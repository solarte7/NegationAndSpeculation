{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#!pip3 install tabulate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import h5py\n",
    "\n",
    "import pickle\n",
    "pickle.HIGHEST_PROTOCOL = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The iput data is a set of  BIO files of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['training', 'testing', 'developing']\n",
    "path    = os.getcwd() + '/BIO_data/'\n",
    "\n",
    "#print(path)\n",
    "\n",
    "for folder in dataset:\n",
    "    files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path + folder):\n",
    "        for file in f:\n",
    "            if file.endswith(\".bio\"):\n",
    "                #print(os.path.join(r, file))\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    data = \"\"\n",
    "    for i, f in enumerate(files):\n",
    "        with open(f) as fp: \n",
    "            data += fp.read()\n",
    "        data += '\\n'\n",
    "    \n",
    "    with open (path + folder + '.bio', 'w') as fp:\n",
    "        fp.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Proccess to Covert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileinput = ['training.bio', 'testing.bio', 'developing.bio']\n",
    "columnas  = [\"word\", \"ner\", \"lema\", \"postag\", \"tokpos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    file = open(filename, encoding=\"utf-8\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunk(data):\n",
    "    results = []\n",
    "    flat_eos = 0\n",
    "    for p, line in enumerate(data):\n",
    "        try:\n",
    "            parsed = dict()\n",
    "            if len(line) == 0:\n",
    "                flat_eos = 0\n",
    "            elif len(line) == 4:\n",
    "                for k, h in enumerate(columnas):\n",
    "                    if k < 4:\n",
    "                        parsed[h] = line[k]\n",
    "                    elif k == 4 and flat_eos == 0:\n",
    "                        parsed[\"tokpos\"] = 'SOS'\n",
    "                        flat_eos = 1\n",
    "                    else:\n",
    "                        parsed[\"tokpos\"] = 'WOS'\n",
    "                        flat_eos = 1\n",
    "            results.append(parsed)\n",
    "        except Exception as e:\n",
    "            print(e, p)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process(file):\n",
    "    file = open_file(file).readlines()\n",
    "    listify = [line.split() for line in file]\n",
    "\n",
    "    data = chunks(listify, int(len(listify) / (mp.cpu_count())))\n",
    "    p = Pool(processes=mp.cpu_count())\n",
    "    results = [p.apply_async(parse_chunk, args=(list(x),)) for x in data]\n",
    "\n",
    "    # wait for results\n",
    "    results = [item.get() for item in results]\n",
    "    results = sum(results, [])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = run_process(path + fileinput[0])\n",
    "\n",
    "df1 = pd.DataFrame(k1, columns=columnas)\n",
    "\n",
    "print(len(df1))\n",
    "#df1 = df1[df1['word'].notna()]\n",
    "#print(df1.loc[13, 'word'])\n",
    "\n",
    "df1.dropna(subset = [\"word\"], inplace= True)\n",
    "\n",
    "print(len(df1))\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df1.to_hdf(fileinput[0] + '.h5', key='df1', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Process of Conversion DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = \"training.bio\"\n",
    "dta1 = pd.read_hdf(name1 + '.h5', 'df1')\n",
    "df1 = dta1[['word', 'ner', 'tokpos']]\n",
    "indi1 = df1.loc[df1['tokpos'] == 'SOS']\n",
    "\n",
    "name2 = \"testing.bio\"\n",
    "dta2 = pd.read_hdf(name2 + '.h5', 'df2')\n",
    "df2 = dta2[['word', 'ner', 'tokpos']]\n",
    "indi2 = df2.loc[df2['tokpos'] == 'SOS']\n",
    "\n",
    "name3 = \"developing.bio\"\n",
    "dta3 = pd.read_hdf(name3 + '.h5', 'df3')\n",
    "df3 = dta3[['word', 'ner', 'tokpos']]\n",
    "indi3 = df3.loc[df3['tokpos'] == 'SOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in indi1.iterrows():\n",
    "    if index != 0:\n",
    "        dta1.at[(index - 1), \"tokpos\"] = 'EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in indi2.iterrows():\n",
    "    if index != 0:\n",
    "        dta2.at[(index - 1), \"tokpos\"] = 'EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in indi3.iterrows():\n",
    "    if index != 0:\n",
    "        dta3.at[(index - 1), \"tokpos\"] = 'EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta1.to_hdf(name1 + '.mod.h5', key='df1', mode='w')\n",
    "\n",
    "dta2.to_hdf(name2 + '.mod.h5', key='df2', mode='w')\n",
    "\n",
    "dta3.to_hdf(name3 + '.mod.h5', key='df3', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
