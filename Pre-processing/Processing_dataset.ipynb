{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing and Characterization a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U future \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import pickle5 as pickle\n",
    "from math import nan\n",
    "from future.utils import iteritems\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#current_path = os.path.abspath(os.path.dirname(__file__))\n",
    "#previus_path = os.path.join(current_path, '../')\n",
    "previus_path = './BIO_data/'\n",
    "\n",
    "train_file = previus_path + 'train.bio.mod.h5'\n",
    "test_file  = previus_path + 'test.bio.mod.h5'\n",
    "dev_file   = previus_path + 'dev.bio.mod.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataf(archiv, key, descp=False):\n",
    "    _dta = pd.read_hdf(archiv, key)\n",
    "    \n",
    "    if descp:\n",
    "        print(df.head())\n",
    "\n",
    "    return _dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_tr = get_dataf(train_file, 'df1')\n",
    "df_tr  = dta_tr[['word', 'ner', '[tokpos]']]\n",
    "print(df_tr['ner'].value_counts())\n",
    "\n",
    "words_tr = list(set(dta_tr[\"word\"].values))\n",
    "n_words_tr = len(words_tr)\n",
    "\n",
    "tags_tr = []\n",
    "for tag in set(dta_tr[\"ner\"].values):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags_tr.append('unk')\n",
    "    else:\n",
    "        tags_tr.append(tag)\n",
    "\n",
    "n_tags_tr = len(tags_tr)\n",
    "\n",
    "print('No. Palabras Unicas: ', n_words_tr)\n",
    "print('    No. Tags Unicas: ', n_tags_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_ts = get_dataf(test_file, 'df2')\n",
    "df_ts  = dta_ts[['word', 'ner', '[tokpos]']]\n",
    "print(df_ts['ner'].value_counts())\n",
    "\n",
    "words_ts = list(set(dta_ts[\"word\"].values))\n",
    "n_words_ts = len(words_ts)\n",
    "\n",
    "tags_ts = []\n",
    "for tag in set(dta_ts[\"ner\"].values):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags_ts.append('unk')\n",
    "    else:\n",
    "        tags_ts.append(tag)\n",
    "        \n",
    "n_tags_ts = len(tags_ts)\n",
    "\n",
    "print('No. Palabras Unicas: ', n_words_ts)\n",
    "print('    No. Tags Unicas: ', n_tags_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the evaluate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_dev = get_dataf(dev_file, 'df3')\n",
    "df_dev  = dta_dev[['word', 'ner', '[tokpos]']]\n",
    "print(df_dev['ner'].value_counts())\n",
    "\n",
    "words_dev = list(set(dta_dev[\"word\"].values))\n",
    "n_words_dev = len(words_dev)\n",
    "\n",
    "tags_dev = []\n",
    "for tag in set(dta_dev[\"ner\"].values):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags_dev.append('unk')\n",
    "    else:\n",
    "        tags_dev.append(tag)\n",
    "        \n",
    "n_tags_dev = len(tags_dev)\n",
    "\n",
    "print('No. Palabras Unicas: ', n_words_dev)\n",
    "print('    No. Tags Unicas: ', n_tags_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint description of all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_tr, df_ts, df_dev]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "    \n",
    "\n",
    "print(df['ner'].value_counts())\n",
    "\n",
    "words = list(set(df[\"word\"].values))\n",
    "n_words = len(words)\n",
    "\n",
    "tags = []\n",
    "for tag in set(df[\"ner\"].values):\n",
    "    if tag is nan or isinstance(tag, float):\n",
    "        tags.append('unk')\n",
    "    else:\n",
    "        tags.append(tag)\n",
    "        \n",
    "n_tags = len(tags)\n",
    "\n",
    "print('No. Palabras Unicas: ', n_words)\n",
    "print('    No. Tags Unicas: ', n_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dtas):\n",
    "    data = []\n",
    "    subdata = []\n",
    "    for index, row in dtas.iterrows():\n",
    "        if row['[tokpos]'] != 'EOS':\n",
    "            subdata.append((row['word'], row['ner']))\n",
    "        else:\n",
    "            subdata.append((row['word'], row['ner']))\n",
    "            data.append(subdata)\n",
    "            subdata = []\n",
    "             \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction Training DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = extract_data(df_tr)\n",
    "print(sentences_train[1:3])\n",
    "\n",
    "with open(\"../vectors/sentences_train.txt\", \"wb\") as fp:\n",
    "    pickle.dump(sentences_train, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = extract_data(df_ts)\n",
    "print(sentences_test[1:3])\n",
    "\n",
    "with open(\"../vectors/sentences_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(sentences_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Extraction Eval DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dev = extract_data(df_dev)\n",
    "print(sentences_dev[1:3])\n",
    "\n",
    "with open(\"../vectors/sentences_dev.txt\", \"wb\") as fp:\n",
    "    pickle.dump(sentences_dev, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of dictionaries of words and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i + 2  for i, w in enumerate(words)}\n",
    "word2idx['-PAD-'] = 0  # The special value used for padding\n",
    "word2idx['-OOV-'] = 1  # The special value used for OOVs\n",
    "\n",
    "tag2idx = {t: i + 2  for i, t in enumerate(tags)}\n",
    "tag2idx['-PAD-'] = 0  # The special value used to padding\n",
    "tag2idx['-OOV-'] = 1  # The special value used to OOVs\n",
    "\n",
    "idx2tag = {v: k for k, v in iteritems(tag2idx)}\n",
    "\n",
    "#np.save('../vectors/word2index.npy', word2idx)\n",
    "#np.save('../vectors/tag2index.npy', tag2idx)\n",
    "#np.save('../vectors/index2tag.npy', idx2tag)\n",
    "\n",
    "print('**** Diccionario de palabras: ****\\n')\n",
    "for key, value in word2idx.items():\n",
    "    if value == 10:\n",
    "        break\n",
    "    else:\n",
    "        print(key, ' : ', value)\n",
    "\n",
    "print('\\n**** Diccionario de tags: ****\\n')\n",
    "for key, value in tag2idx.items():\n",
    "    if value == 10:\n",
    "        break\n",
    "    else:\n",
    "        print(key, ' : ', value)\n",
    "\n",
    "print('\\n**** array de tags: ****\\n')\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the maximum length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_train = max([len(s) for s in sentences_train])\n",
    "print('longitud oraciones entraneminto: ',  maxlen_train)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences_train], bins=50)\n",
    "plt.title('Numero de palabras por oracion')\n",
    "plt.xlabel('Longitud Oracion')\n",
    "plt.ylabel('# oraciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_test = max([len(s) for s in sentences_test])\n",
    "print('\\nlongitud oraciones pruebas: ',  maxlen_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences_test], bins=50)\n",
    "plt.title('Numero de tokens por oracion')\n",
    "plt.xlabel('Longitud Oracion')\n",
    "plt.ylabel('# oraciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_dev = max([len(s) for s in sentences_dev])\n",
    "print('\\nlongitud oraciones Evaluaci√≥n: ',  maxlen_dev)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in sentences_dev], bins=50)\n",
    "plt.title('Numero de tokens por oracion')\n",
    "plt.xlabel('Longitud Oracion')\n",
    "plt.ylabel('# oraciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([maxlen_train, maxlen_test, maxlen_dev])\n",
    "\n",
    "print('\\nlongitud oraciones: ', maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting training dataset in classes and categories mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[word2idx[w[0]] for w in s] for s in sentences_train]\n",
    "X_train = pad_sequences(maxlen=maxlen, sequences=X_train, padding=\"post\",value=word2idx[\"-PAD-\"])\n",
    "\n",
    "print(X_train[0])\n",
    "\n",
    "#np.save('../vectors/X_train.npy', X_train)\n",
    "\n",
    "#X_train_cat = [to_categorical(i, num_classes=n_words+2) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [[tag2idx[w[1]] for w in s] for s in sentences_train]\n",
    "y_train = pad_sequences(maxlen=maxlen, sequences=y_train, padding=\"post\", value=tag2idx[\"-PAD-\"])\n",
    "y_train = [to_categorical(i, num_classes=n_tags+2) for i in y_train]\n",
    "\n",
    "print(y_train[0][0])\n",
    "print(y_train[0])\n",
    "\n",
    "#np.save('../vectors/y_train.npy', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting testing dataset in classes and categories mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[word2idx[w[0]] for w in s] for s in sentences_test]\n",
    "X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=word2idx[\"-PAD-\"])\n",
    "\n",
    "print(X_test[0])\n",
    "\n",
    "#np.save('../vectors/X_test.npy', X_test)\n",
    "\n",
    "#X_test_cat = [to_categorical(i, num_classes=n_words+2) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[tag2idx[w[1]] for w in s] for s in sentences_test]\n",
    "y_test = pad_sequences(maxlen=maxlen, sequences=y_test, padding=\"post\", value=tag2idx[\"-PAD-\"])\n",
    "y_test = [to_categorical(i, num_classes=n_tags+2) for i in y_test]\n",
    "\n",
    "print(y_test[0][0])\n",
    "print(y_test[0])\n",
    "\n",
    "#np.save('../vectors/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting eval dataset in classes and categories mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = [[word2idx[w[0]] for w in s] for s in sentences_dev]\n",
    "X_dev = pad_sequences(maxlen=maxlen, sequences=X_dev, padding=\"post\",value=word2idx[\"-PAD-\"])\n",
    "\n",
    "print(X_dev[0])\n",
    "\n",
    "#np.save('../vectors/X_dev.npy', X_dev)\n",
    "\n",
    "#X_dev_cat = [to_categorical(i, num_classes=n_words+2) for i in X_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = [[tag2idx[w[1]] for w in s] for s in sentences_dev]\n",
    "\n",
    "print(type(y_dev))\n",
    "print(len(y_dev[0]))\n",
    "\n",
    "y_dev = pad_sequences(maxlen=maxlen, sequences=y_dev, padding=\"post\", value=tag2idx[\"-PAD-\"])\n",
    "\n",
    "print(type(y_dev))\n",
    "#print(y_dev)\n",
    "\n",
    "y_dev = [to_categorical(i, num_classes=n_tags+2) for i in y_dev]\n",
    "\n",
    "print(type(y_dev))\n",
    "#print(y_dev)\n",
    "\n",
    "print(y_dev[0][0])\n",
    "print(y_dev[0])\n",
    "\n",
    "#np.save('../vectors/y_dev.npy', y_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
