{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all, you must install next software requirements\n",
    "\n",
    "#!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n",
    "#!pip install seqeval\n",
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('libs/')\n",
    "\n",
    "import datetime, os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# ****** DEFINICION DE PARAMETROS *********\n",
    "MAX_LEN        = 348\n",
    "NUM_LABELS     = 12\n",
    "WORD_PAD_TOKEN = 0\n",
    "\n",
    "#ESPECIAL_TOKEN = 9\n",
    "#SEP_TOKEN      = 10\n",
    "#PAD_TOKEN      = 11\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "#MODEL         = 'model'\n",
    "log_dir       = \"saved_model/logs/model/\"\n",
    "save_dir      = \"saved_model/model/\" \n",
    "\n",
    "le_dicti = {'B-NEG': 0, 'B-NSCO': 1, 'B-UNC': 2, 'B-USCO': 3, 'I-NEG': 4, 'I-NSCO': 5, 'I-UNC': 6, 'I-USCO': 7, 'O': 8, '[CLS]': 9, '[SEP]': 10, '[PAD]': 11}\n",
    "\n",
    "le_dict = {}\n",
    "for key in le_dicti:\n",
    "    #print(key, '->', le_dict[key])\n",
    "    le_dict[le_dicti[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(sentences, tags, in_ou_put):\n",
    "    input_id_list       = []\n",
    "    attention_mask_list = [] \n",
    "    token_type_id_list  = []\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = []\n",
    "    else:\n",
    "        label_id_list   = 0\n",
    "    \n",
    "    for x,y in tqdm(zip(sentences,tags),total=len(tags)):\n",
    "        tokens = []\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_ids = []\n",
    "        \n",
    "        for word, label in zip(x, y):\n",
    "            word_tokens = tokenizer.tokenize(str(word))\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, \n",
    "            # and padding ids for the remaining tokens\n",
    "            if in_ou_put == 1:\n",
    "                #label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "                label_ids.extend([label] + [SEP_TOKEN] * (len(word_tokens) - 1))\n",
    "        \n",
    "        # special_tokens_count =  2\n",
    "        \n",
    "        #if len(tokens) > LEN_SENTS - special_tokens_count:\n",
    "        #    tokens = tokens[: (LEN_SENTS - special_tokens_count)]\n",
    "\n",
    "        #    if in_ou_put == 1:\n",
    "        #        label_ids = label_ids[: (LEN_SENTS - special_tokens_count)]\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            #label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "            label_ids = [ESPECIAL_TOKEN] + label_ids + [ESPECIAL_TOKEN]\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=MAX_LEN)\n",
    "        \n",
    "        input_ids       = inputs[\"input_ids\"]\n",
    "        token_type_ids  = inputs[\"token_type_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        \n",
    "        #print(attention_masks)\n",
    "        #attention_masks = [17] + [1] * (len(input_ids)-2) + [17]\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        attention_mask_list.append(attention_masks)\n",
    "        input_id_list.append(input_ids)\n",
    "        token_type_id_list.append(token_type_ids)\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_id_list.append(label_ids)\n",
    "\n",
    "    input_id_list       = pad_sequences(maxlen=MAX_LEN, sequences=input_id_list,       dtype=\"int32\", padding=\"post\", value=WORD_PAD_TOKEN)\n",
    "    token_type_id_list  = pad_sequences(maxlen=MAX_LEN, sequences=token_type_id_list,  dtype=\"int32\", padding=\"post\")\n",
    "    attention_mask_list = pad_sequences(maxlen=MAX_LEN, sequences=attention_mask_list, dtype=\"int32\", padding=\"post\")\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = pad_sequences(maxlen=MAX_LEN, sequences=label_id_list, dtype=\"int32\", padding=\"post\", value=PAD_TOKEN)\n",
    "        #label_id_list   = [to_categorical(i, num_classes=num_labels, dtype =\"int32\") for i in label_id_list]\n",
    "        #label_id_list   = np.array(label_id_list)\n",
    "\n",
    "    return input_id_list, token_type_id_list, attention_mask_list, label_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "config = config_class.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "print(\"\\nLoading the model for negation and speculation detection ...\")\n",
    "\n",
    "new_model = tf.saved_model.load(save_dir)\n",
    "\n",
    "print (\"\\nModel loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Sentences must be previously tokenized (e.g Using Spacy for spanish)\n",
    "#The model receives a tokenized sentence and returns a negation or speculation label for each token.\n",
    "#Next sentences have been proviously tokenized. \n",
    "#It is recommended to use  Spacy tokenizerfor spanish language.\n",
    "\n",
    "negation_samples = [\n",
    "    \"No dolor toráxico.\".split(),\n",
    "    \"Paciente con probable carcinoma pulmonar.\".split(),\n",
    "    \"inflamacion aguda negativa.\".split(),\n",
    "    \"helicobacter pylori negativo.\".split(),\n",
    "    \"negativo para malignidad.\".split(),\n",
    "    \"No se puede descartar cáncer de pulmón .\".split(),\n",
    "    \"helicobacter pylori negativo.\".split(),\n",
    "    \"Test negativo para malignidad.\".split() \n",
    "    \n",
    "]\n",
    "\n",
    "dummy_y_train = []\n",
    "#dummy_y_train = ['b-neg', 'scope', 'o'....]\n",
    "\n",
    "for snt in negation_samples:\n",
    "    senti = []\n",
    "    for wds in snt:\n",
    "        senti.append('-PAD-')\n",
    "    \n",
    "    dummy_y_train.append(senti)\n",
    "\n",
    "\n",
    "demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train, label_ids_train = convert_to_input(negation_samples, dummy_y_train, 0)\n",
    "#demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train, label_ids_train = convert_to_input(negation_samples, dummy_y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "demo_prediction = new_model([demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train])\n",
    "\n",
    "demo_pred_tags = np.argmax(demo_prediction, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y_pred = logits_to_tokens(demo_pred_tags, le_dict)\n",
    "#print(demo_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h, oracc in enumerate(negation_samples):\n",
    "    #heads = oracc\n",
    "    #if h == 0:\n",
    "    tokensito = []\n",
    "    for wordi in oracc:\n",
    "        wordi_tokens = tokenizer.tokenize(str(wordi))\n",
    "        tokensito.extend(wordi_tokens)\n",
    "\n",
    "    print(oracc)\n",
    "    #print(tokensito)\n",
    "    #print(demo_y_pred[h])\n",
    "    heads = tokensito\n",
    "    body  = [demo_y_pred[h][1:len(tokensito)+1]]\n",
    "    \n",
    "    display(HTML(\"<div style='overflow-x: auto; white-space: nowrap;'>\" + \n",
    "                 tabulate(body, headers=heads, tablefmt=\"html\") + \n",
    "                 \"</div>\"))\n",
    "    \n",
    "   \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
