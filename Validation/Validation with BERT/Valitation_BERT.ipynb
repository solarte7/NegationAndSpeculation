{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all, you must install next software requirements\n",
    "\n",
    "#!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n",
    "#!pip install seqeval\n",
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('libs/')\n",
    "\n",
    "import datetime, os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report as eskclarep\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# ****** DEFINICION DE PARAMETROS *********\n",
    "MAX_LEN        = 348\n",
    "NUM_LABELS     = 12\n",
    "WORD_PAD_TOKEN = 0\n",
    "\n",
    "ESPECIAL_TOKEN = 9\n",
    "SEP_TOKEN      = 10\n",
    "PAD_TOKEN      = 11\n",
    "WORD_PAD_TOKEN = 0\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "#MODEL         = 'model'\n",
    "log_dir       = \"saved_model/logs/model/\"\n",
    "save_dir      = \"saved_model/model/\" \n",
    "\n",
    "le_dicti = {'B-NEG': 0, 'B-NSCO': 1, 'B-UNC': 2, 'B-USCO': 3, 'I-NEG': 4, 'I-NSCO': 5, 'I-UNC': 6, 'I-USCO': 7, 'O': 8, '[CLS]': 9, '[SEP]': 10, '[PAD]': 11}\n",
    "\n",
    "le_dict = {}\n",
    "for key in le_dicti:\n",
    "    #print(key, '->', le_dict[key])\n",
    "    le_dict[le_dicti[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar CSV\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def process_csv(data_path):    \n",
    "    df = pd.read_csv(data_path, encoding=\"utf-8\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "    sentences_l = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    sentences = sentences_l.tolist()\n",
    "    tag_l = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    tag = tag_l.tolist()\n",
    "    return sentences, tag, enc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_csv = \"sentences_test.csv\"\n",
    "X_test, y_test, enc_tag_test   = process_csv(test_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0])\n",
    "print(y_test[0])\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(sentences, tags, in_ou_put):\n",
    "    input_id_list       = []\n",
    "    attention_mask_list = [] \n",
    "    token_type_id_list  = []\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = []\n",
    "    else:\n",
    "        label_id_list   = 0\n",
    "    \n",
    "    for x,y in tqdm(zip(sentences,tags),total=len(tags)):\n",
    "        tokens = []\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_ids = []\n",
    "        \n",
    "        for word, label in zip(x, y):\n",
    "            word_tokens = tokenizer.tokenize(str(word))\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, \n",
    "            # and padding ids for the remaining tokens\n",
    "            if in_ou_put == 1:\n",
    "                #label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "                label_ids.extend([label] + [SEP_TOKEN] * (len(word_tokens) - 1))\n",
    "        \n",
    "        # special_tokens_count =  2\n",
    "        \n",
    "        #if len(tokens) > LEN_SENTS - special_tokens_count:\n",
    "        #    tokens = tokens[: (LEN_SENTS - special_tokens_count)]\n",
    "\n",
    "        #    if in_ou_put == 1:\n",
    "        #        label_ids = label_ids[: (LEN_SENTS - special_tokens_count)]\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            #label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "            label_ids = [ESPECIAL_TOKEN] + label_ids + [ESPECIAL_TOKEN]\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=MAX_LEN)\n",
    "        \n",
    "        input_ids       = inputs[\"input_ids\"]\n",
    "        token_type_ids  = inputs[\"token_type_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        \n",
    "        #print(attention_masks)\n",
    "        #attention_masks = [17] + [1] * (len(input_ids)-2) + [17]\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        attention_mask_list.append(attention_masks)\n",
    "        input_id_list.append(input_ids)\n",
    "        token_type_id_list.append(token_type_ids)\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_id_list.append(label_ids)\n",
    "\n",
    "    input_id_list       = pad_sequences(maxlen=MAX_LEN, sequences=input_id_list,       dtype=\"int32\", padding=\"post\", value=WORD_PAD_TOKEN)\n",
    "    token_type_id_list  = pad_sequences(maxlen=MAX_LEN, sequences=token_type_id_list,  dtype=\"int32\", padding=\"post\")\n",
    "    attention_mask_list = pad_sequences(maxlen=MAX_LEN, sequences=attention_mask_list, dtype=\"int32\", padding=\"post\")\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = pad_sequences(maxlen=MAX_LEN, sequences=label_id_list, dtype=\"int32\", padding=\"post\", value=PAD_TOKEN)\n",
    "        #label_id_list   = [to_categorical(i, num_classes=num_labels, dtype =\"int32\") for i in label_id_list]\n",
    "        #label_id_list   = np.array(label_id_list)\n",
    "\n",
    "    return input_id_list, token_type_id_list, attention_mask_list, label_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "config = config_class.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "print(\"\\nLoading the model for negation and speculation detection ...\")\n",
    "\n",
    "new_model = tf.saved_model.load(save_dir)\n",
    "\n",
    "print (\"\\nModel loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Sentences must be previously tokenized (e.g Using Spacy for spanish)\n",
    "#The model receives a tokenized sentence and returns a negation or speculation label for each token.\n",
    "#Next sentences have been proviously tokenized. \n",
    "#It is recommended to use  Spacy tokenizerfor spanish language.\n",
    "\n",
    "    \n",
    "#demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train, label_ids_train = convert_to_input(X_test, y_test, 1)\n",
    "#demo_input_ids_train1, demo_token_ids_train1, demo_attention_masks_train1, label_ids_train1 = convert_to_input(X_test[0:100], y_test[0:100], 1)\n",
    "#demo_input_ids_train2, demo_token_ids_train2, demo_attention_masks_train2, label_ids_train2 = convert_to_input(X_test[100:200], y_test[100:200], 1)\n",
    "#demo_input_ids_train3, demo_token_ids_train3, demo_attention_masks_train3, label_ids_train3 = convert_to_input(X_test[200:300], y_test[200:300], 1)\n",
    "#demo_input_ids_train4, demo_token_ids_train4, demo_attention_masks_train4, label_ids_train4 = convert_to_input(X_test[300:400], y_test[300:400], 1)\n",
    "#demo_input_ids_train5, demo_token_ids_train5, demo_attention_masks_train5, label_ids_train5 = convert_to_input(X_test[400:500], y_test[400:500], 1)\n",
    "#demo_input_ids_train6, demo_token_ids_train6, demo_attention_masks_train6, label_ids_train6 = convert_to_input(X_test[500:600], y_test[500:600], 1)\n",
    "#demo_input_ids_train7, demo_token_ids_train7, demo_attention_masks_train7, label_ids_train7 = convert_to_input(X_test[600:700], y_test[600:700], 1)\n",
    "#demo_input_ids_train8, demo_token_ids_train8, demo_attention_masks_train8, label_ids_train8 = convert_to_input(X_test[700:776], y_test[700:776], 1)\n",
    "\n",
    "demo_input_ids_train1, demo_token_ids_train1, demo_attention_masks_train1, label_ids_train1 = convert_to_input(X_test[0:150], y_test[0:150], 1)\n",
    "demo_input_ids_train2, demo_token_ids_train2, demo_attention_masks_train2, label_ids_train2 = convert_to_input(X_test[150:300], y_test[150:300], 1)\n",
    "demo_input_ids_train3, demo_token_ids_train3, demo_attention_masks_train3, label_ids_train3 = convert_to_input(X_test[300:450], y_test[300:450], 1)\n",
    "demo_input_ids_train4, demo_token_ids_train4, demo_attention_masks_train4, label_ids_train4 = convert_to_input(X_test[450:600], y_test[450:600], 1)\n",
    "demo_input_ids_train5, demo_token_ids_train5, demo_attention_masks_train5, label_ids_train5 = convert_to_input(X_test[600:750], y_test[600:750], 1)\n",
    "demo_input_ids_train6, demo_token_ids_train6, demo_attention_masks_train6, label_ids_train6 = convert_to_input(X_test[750:900], y_test[750:900], 1)\n",
    "demo_input_ids_train7, demo_token_ids_train7, demo_attention_masks_train7, label_ids_train7 = convert_to_input(X_test[900:1050], y_test[900:1050], 1)\n",
    "demo_input_ids_train8, demo_token_ids_train8, demo_attention_masks_train8, label_ids_train8 = convert_to_input(X_test[1050:1200], y_test[1050:1200], 1)\n",
    "demo_input_ids_train9, demo_token_ids_train9, demo_attention_masks_train9, label_ids_train9 = convert_to_input(X_test[1200:1350], y_test[1200:1350], 1)\n",
    "demo_input_ids_train10, demo_token_ids_train10, demo_attention_masks_train10, label_ids_train10 = convert_to_input(X_test[1350:1500], y_test[1350:1500], 1)\n",
    "demo_input_ids_train11, demo_token_ids_train11, demo_attention_masks_train11, label_ids_train11 = convert_to_input(X_test[1500:1587], y_test[1500:1587], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "#demo_prediction = new_model([demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train])\n",
    "#demo_pred_tags = np.argmax(demo_prediction, -1)\n",
    "\n",
    "demo_prediction1 = new_model([demo_input_ids_train1, demo_token_ids_train1, demo_attention_masks_train1])\n",
    "demo_prediction2 = new_model([demo_input_ids_train2, demo_token_ids_train2, demo_attention_masks_train2])\n",
    "demo_prediction3 = new_model([demo_input_ids_train3, demo_token_ids_train3, demo_attention_masks_train3])\n",
    "demo_prediction4 = new_model([demo_input_ids_train4, demo_token_ids_train4, demo_attention_masks_train4])\n",
    "demo_prediction5 = new_model([demo_input_ids_train5, demo_token_ids_train5, demo_attention_masks_train5])\n",
    "demo_prediction6 = new_model([demo_input_ids_train6, demo_token_ids_train6, demo_attention_masks_train6])\n",
    "demo_prediction7 = new_model([demo_input_ids_train7, demo_token_ids_train7, demo_attention_masks_train7])\n",
    "demo_prediction8 = new_model([demo_input_ids_train8, demo_token_ids_train8, demo_attention_masks_train8])\n",
    "\n",
    "demo_prediction9 = new_model([demo_input_ids_train9, demo_token_ids_train9, demo_attention_masks_train9])\n",
    "demo_prediction10 = new_model([demo_input_ids_train10, demo_token_ids_train10, demo_attention_masks_train10])\n",
    "demo_prediction11 = new_model([demo_input_ids_train11, demo_token_ids_train11, demo_attention_masks_train11])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_prediction1.shape)\n",
    "print(demo_prediction2.shape)\n",
    "print(demo_prediction3.shape)\n",
    "print(demo_prediction4.shape)\n",
    "print(demo_prediction5.shape)\n",
    "print(demo_prediction6.shape)\n",
    "print(demo_prediction7.shape)\n",
    "print(demo_prediction8.shape)\n",
    "print(demo_prediction9.shape)\n",
    "print(demo_prediction10.shape)\n",
    "print(demo_prediction11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pred_tags1 = np.argmax(demo_prediction1, -1)\n",
    "demo_pred_tags2 = np.argmax(demo_prediction2, -1)\n",
    "demo_pred_tags3 = np.argmax(demo_prediction3, -1)\n",
    "demo_pred_tags4 = np.argmax(demo_prediction4, -1)\n",
    "demo_pred_tags5 = np.argmax(demo_prediction5, -1)\n",
    "demo_pred_tags6 = np.argmax(demo_prediction6, -1)\n",
    "demo_pred_tags7 = np.argmax(demo_prediction7, -1)\n",
    "demo_pred_tags8 = np.argmax(demo_prediction8, -1)\n",
    "demo_pred_tags9 = np.argmax(demo_prediction9, -1)\n",
    "demo_pred_tags10 = np.argmax(demo_prediction10, -1)\n",
    "demo_pred_tags11 = np.argmax(demo_prediction11, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_pred_tags1.shape)\n",
    "print(demo_pred_tags2.shape)\n",
    "print(demo_pred_tags3.shape)\n",
    "print(demo_pred_tags4.shape)\n",
    "print(demo_pred_tags5.shape)\n",
    "print(demo_pred_tags6.shape)\n",
    "print(demo_pred_tags7.shape)\n",
    "print(demo_pred_tags8.shape)\n",
    "print(demo_pred_tags9.shape)\n",
    "print(demo_pred_tags10.shape)\n",
    "print(demo_pred_tags11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pred_tags_0 = np.vstack((demo_pred_tags1, demo_pred_tags2))\n",
    "demo_pred_tags_1 = np.vstack((demo_pred_tags_0, demo_pred_tags3))\n",
    "demo_pred_tags_2 = np.vstack((demo_pred_tags_1, demo_pred_tags4))\n",
    "demo_pred_tags_3 = np.vstack((demo_pred_tags_2, demo_pred_tags5))\n",
    "demo_pred_tags_4 = np.vstack((demo_pred_tags_3, demo_pred_tags6))\n",
    "demo_pred_tags_5 = np.vstack((demo_pred_tags_4, demo_pred_tags7))\n",
    "demo_pred_tags_6 = np.vstack((demo_pred_tags_5, demo_pred_tags8))\n",
    "demo_pred_tags_7 = np.vstack((demo_pred_tags_6, demo_pred_tags9))\n",
    "demo_pred_tags_8 = np.vstack((demo_pred_tags_7, demo_pred_tags10))\n",
    "demo_pred_tags   = np.vstack((demo_pred_tags_8, demo_pred_tags11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_pred_tags_0.shape)\n",
    "print(demo_pred_tags_1.shape)\n",
    "print(demo_pred_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y_pred = logits_to_tokens(demo_pred_tags, le_dict)\n",
    "print(demo_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_y_pred = logits_to_tokens(demo_pred_tags, le_dict)\n",
    "#print(demo_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(label_ids_train1))\n",
    "print(label_ids_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids_train_0 = np.vstack((label_ids_train1, label_ids_train2))\n",
    "label_ids_train_1 = np.vstack((label_ids_train_0, label_ids_train3))\n",
    "label_ids_train_2 = np.vstack((label_ids_train_1, label_ids_train4))\n",
    "label_ids_train_3 = np.vstack((label_ids_train_2, label_ids_train5))\n",
    "label_ids_train_4 = np.vstack((label_ids_train_3, label_ids_train6))\n",
    "label_ids_train_5 = np.vstack((label_ids_train_4, label_ids_train7))\n",
    "label_ids_train_6 = np.vstack((label_ids_train_5, label_ids_train8))\n",
    "label_ids_train_7 = np.vstack((label_ids_train_6, label_ids_train9))\n",
    "label_ids_train_8 = np.vstack((label_ids_train_7, label_ids_train10))\n",
    "label_ids_train   = np.vstack((label_ids_train_8, label_ids_train11))\n",
    "print(label_ids_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = logits_to_tokens(label_ids_train, le_dict)#label_ids_train\n",
    "\n",
    "print(y_true[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = sum(y_true, [])\n",
    "li2 = sum(demo_y_pred, [])#demo_y_pred\n",
    "\n",
    "results = pd.DataFrame(columns=['Expected', 'Predicted'])\n",
    "\n",
    "results['Expected']  = li1\n",
    "results['Predicted'] = li2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision: {:.1%}\".format(precision_score(y_true, demo_y_pred)))\n",
    "print(\"   recall: {:.1%}\".format(recall_score(y_true,    demo_y_pred)))\n",
    "print(\" accuracy: {:.1%}\".format(accuracy_score(y_true,  demo_y_pred)))\n",
    "print(\" F1-score: {:.1%}\".format(f1_score(y_true,        demo_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = eskclarep(results['Expected'], results['Predicted'])\n",
    "print(report_to_df(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
